{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install box2d pygame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i5JSb7zdX-k",
        "outputId": "cf2f351e-d63b-436e-bf76-1afc8f1468ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting box2d\n",
            "  Downloading Box2D-2.3.10-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 29.3 MB/s \n",
            "\u001b[?25hCollecting pygame\n",
            "  Downloading pygame-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 1.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame, box2d\n",
            "Successfully installed box2d-2.3.10 pygame-2.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TJcFLzK5dA2C"
      },
      "outputs": [],
      "source": [
        "from gym import make\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import Adam\n",
        "from collections import deque, namedtuple\n",
        "import random\n",
        "import copy\n",
        "\n",
        "GAMMA = 0.99\n",
        "INITIAL_STEPS = 1024\n",
        "TRANSITIONS = 500000\n",
        "STEPS_PER_UPDATE = 4\n",
        "STEPS_PER_TARGET_UPDATE = STEPS_PER_UPDATE * 1000\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 5e-4\n",
        "SEED = 42\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QModel(nn.Module):\n",
        "    def __init__(self, observation_space, action_space, hidden_space=64, seed=SEED):\n",
        "        super(QModel, self).__init__()\n",
        "        \n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "        self.hidden_space = hidden_space\n",
        "        \n",
        "        self.linear1 = nn.Linear(observation_space, hidden_space)\n",
        "        self.linear2 = nn.Linear(hidden_space, hidden_space)\n",
        "        self.linear3 = nn.Linear(hidden_space, action_space)\n",
        "        \n",
        "    def forward(self, state):\n",
        "        res = F.relu(self.linear1(state))\n",
        "        res = F.relu(self.linear2(res))\n",
        "        return self.linear3(res)"
      ],
      "metadata": {
        "id": "7NKld_YndFuA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, maxlen=10000, seed=SEED):\n",
        "        \n",
        "        self.buffer = deque(maxlen=maxlen)\n",
        "        self.Transition = namedtuple(\"Transition\", field_names=[\"state\", \"action\", \"next_state\", \"reward\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "        \n",
        "    def add(self, transition):\n",
        "        transition = self.Transition(*transition)\n",
        "        self.buffer.append(transition)\n",
        "        \n",
        "    def sample(self, batch_size=512):\n",
        "        transitions = random.sample(self.buffer, k=batch_size)\n",
        "        \n",
        "        #size of all: [batch_size, 1]\n",
        "        states = torch.from_numpy(np.vstack([t.state for t in transitions if t is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([t.action for t in transitions if t is not None])).long().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([t.next_state for t in transitions if t is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([t.reward for t in transitions if t is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([t.done for t in transitions if t is not None]).astype(np.uint8)).float().to(device)\n",
        "  \n",
        "        return (states, actions, next_states, rewards, dones)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "HPv4Mpb2dQ5p"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.steps = 0 # Do not change\n",
        "        \n",
        "        self.model = QModel(state_dim, action_dim).to(device)\n",
        "        self.target_model = QModel(state_dim, action_dim).to(device)\n",
        "        self.optimizer = Adam(self.model.parameters())\n",
        "        self.buffer = ReplayBuffer()\n",
        "\n",
        "    def consume_transition(self, transition):\n",
        "        self.buffer.add(transition)\n",
        "        \n",
        "    def sample_batch(self):\n",
        "        return self.buffer.sample()\n",
        "        \n",
        "    def train_step(self, batch):\n",
        "        states, actions, next_states, rewards, dones = batch\n",
        "        \n",
        "        q_next = self.target_model(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        q_target = rewards + GAMMA * q_next * (1 - dones)\n",
        "        q_pred = self.model(states).gather(1, actions)\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        loss = F.mse_loss(q_pred, q_target)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def update_target_network(self):\n",
        "        for target_param, local_param in zip(self.target_model.parameters(), self.model.parameters()):\n",
        "            target_param.data.copy_(local_param.data)\n",
        "\n",
        "    def act(self, state, target=False):\n",
        "        # Compute an action. Do not forget to turn state to a Tensor and then turn an action to a numpy array.\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        \n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            action = np.argmax(self.model(state).cpu().data.numpy())\n",
        "        self.model.train()\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update(self, transition):\n",
        "        # You don't need to change this\n",
        "        self.consume_transition(transition)\n",
        "        if self.steps % STEPS_PER_UPDATE == 0:\n",
        "            batch = self.sample_batch()\n",
        "            self.train_step(batch)\n",
        "        if self.steps % STEPS_PER_TARGET_UPDATE == 0:\n",
        "            self.update_target_network()\n",
        "        self.steps += 1\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.model.state_dict(), \"agent.pt\")"
      ],
      "metadata": {
        "id": "4565nZCkdSMU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_policy(agent, episodes=5):\n",
        "    env = make(\"LunarLander-v2\")\n",
        "    returns = []\n",
        "    for _ in range(episodes):\n",
        "        done = False\n",
        "        state = env.reset()\n",
        "        total_reward = 0.\n",
        "        \n",
        "        while not done:\n",
        "            state, reward, done, _ = env.step(agent.act(state))\n",
        "            total_reward += reward\n",
        "        returns.append(total_reward)\n",
        "    return returns"
      ],
      "metadata": {
        "id": "EZKH_WEVdUFB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = make(\"LunarLander-v2\")\n",
        "dqn = DQN(state_dim=env.observation_space.shape[0], action_dim=env.action_space.n)\n",
        "eps = 0.1\n",
        "state = env.reset()\n",
        "\n",
        "for _ in range(INITIAL_STEPS):\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    dqn.consume_transition((state, action, next_state, reward, done))\n",
        "\n",
        "    state = next_state if not done else env.reset()\n",
        "\n",
        "best_mean_reward = -1000\n",
        "for i in range(TRANSITIONS):\n",
        "    #Epsilon-greedy policy\n",
        "    if random.random() < eps:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        action = dqn.act(state)\n",
        "\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    dqn.update((state, action, next_state, reward, done))\n",
        "\n",
        "    state = next_state if not done else env.reset()\n",
        "\n",
        "    if (i + 1) % (TRANSITIONS//100) == 0:\n",
        "        rewards = evaluate_policy(dqn, 5)\n",
        "        mean_reward = np.mean(rewards)\n",
        "        print(f\"Step: {i+1}, Reward mean: {mean_reward}, Reward std: {np.std(rewards)}\")\n",
        "        if mean_reward > best_mean_reward:\n",
        "            dqn.save()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLeq8jbpdV1p",
        "outputId": "21f95e91-9760-4d2f-a7d6-78ba06457ddc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 5000, Reward mean: -367.9674015949731, Reward std: 29.524392826526544\n",
            "Step: 10000, Reward mean: -440.44014958029345, Reward std: 33.06503109223744\n",
            "Step: 15000, Reward mean: -31.922638518214775, Reward std: 143.53521877145042\n",
            "Step: 20000, Reward mean: -212.3490438740063, Reward std: 237.62674986599538\n",
            "Step: 25000, Reward mean: -181.1844034967929, Reward std: 21.071855542733584\n",
            "Step: 30000, Reward mean: -135.50893373644195, Reward std: 23.968120759853253\n",
            "Step: 35000, Reward mean: -22.482891708550074, Reward std: 142.4344778423344\n",
            "Step: 40000, Reward mean: -119.3590210460023, Reward std: 3.932491472523406\n",
            "Step: 45000, Reward mean: -94.7109614264754, Reward std: 54.645624307772444\n",
            "Step: 50000, Reward mean: -104.40412917368037, Reward std: 12.54118094127263\n",
            "Step: 55000, Reward mean: -172.12328881074603, Reward std: 24.721704369222316\n",
            "Step: 60000, Reward mean: -111.68515604908528, Reward std: 23.84088981188873\n",
            "Step: 65000, Reward mean: -73.05413311937119, Reward std: 37.41642118806534\n",
            "Step: 70000, Reward mean: -105.60525565201108, Reward std: 60.98654662660459\n",
            "Step: 75000, Reward mean: -116.17159879563455, Reward std: 11.390433241516487\n",
            "Step: 80000, Reward mean: -108.02624457594827, Reward std: 81.58350191273699\n",
            "Step: 85000, Reward mean: -109.14700901360395, Reward std: 60.56156210102317\n",
            "Step: 90000, Reward mean: -95.28854248793598, Reward std: 99.5031856945088\n",
            "Step: 95000, Reward mean: -149.45885532230074, Reward std: 59.98414452699085\n",
            "Step: 100000, Reward mean: -88.46121044827821, Reward std: 29.278350969277597\n",
            "Step: 105000, Reward mean: -94.08479222331623, Reward std: 53.37147273077811\n",
            "Step: 110000, Reward mean: -30.12384116461186, Reward std: 42.32603884895632\n",
            "Step: 115000, Reward mean: -79.22052663195696, Reward std: 66.11212907652887\n",
            "Step: 120000, Reward mean: -83.71574766094024, Reward std: 46.5088755665414\n",
            "Step: 125000, Reward mean: -40.11043611815335, Reward std: 27.338924644293076\n",
            "Step: 130000, Reward mean: -55.75127640414237, Reward std: 53.30329781249075\n",
            "Step: 135000, Reward mean: -35.64798504042254, Reward std: 16.59135690102183\n",
            "Step: 140000, Reward mean: -78.82999678769377, Reward std: 43.05837619598315\n",
            "Step: 145000, Reward mean: -30.651246572439636, Reward std: 18.878932658296353\n",
            "Step: 150000, Reward mean: -29.44859115803763, Reward std: 31.107695760478627\n",
            "Step: 155000, Reward mean: -20.42480488146409, Reward std: 35.840150280700385\n",
            "Step: 160000, Reward mean: -67.26720456036544, Reward std: 93.86253250019107\n",
            "Step: 165000, Reward mean: -51.67994173754446, Reward std: 30.809667756435296\n",
            "Step: 170000, Reward mean: -40.081618787890136, Reward std: 19.16820987222476\n",
            "Step: 175000, Reward mean: -45.97151213557315, Reward std: 4.921115980634048\n",
            "Step: 180000, Reward mean: -13.127227764174961, Reward std: 68.86585397597344\n",
            "Step: 185000, Reward mean: -41.32321061666354, Reward std: 23.70377160431438\n",
            "Step: 190000, Reward mean: -40.29330177132236, Reward std: 55.365470364579025\n",
            "Step: 195000, Reward mean: -53.404153329387874, Reward std: 33.96779432447732\n",
            "Step: 200000, Reward mean: -23.198430661625263, Reward std: 84.40855334633825\n",
            "Step: 205000, Reward mean: -29.7297473988741, Reward std: 29.789008312062094\n",
            "Step: 210000, Reward mean: -54.000797422794335, Reward std: 162.9478825678215\n",
            "Step: 215000, Reward mean: 71.95381472097237, Reward std: 76.8367385122077\n",
            "Step: 220000, Reward mean: -75.75153502392448, Reward std: 253.28670307933172\n",
            "Step: 225000, Reward mean: 31.06674506624723, Reward std: 298.6200192256978\n",
            "Step: 230000, Reward mean: 111.69234192513063, Reward std: 128.78472973404712\n",
            "Step: 235000, Reward mean: 146.6625718783977, Reward std: 101.48986261348841\n",
            "Step: 240000, Reward mean: 119.56844916751758, Reward std: 134.37394935021325\n",
            "Step: 245000, Reward mean: -45.65806800578825, Reward std: 13.076220314193371\n",
            "Step: 250000, Reward mean: 167.566562130249, Reward std: 105.79953942133896\n",
            "Step: 255000, Reward mean: 153.5031454578897, Reward std: 82.60883889654392\n",
            "Step: 260000, Reward mean: 177.89247537534928, Reward std: 112.70802598008508\n",
            "Step: 265000, Reward mean: 207.89203024768776, Reward std: 98.25579550682386\n",
            "Step: 270000, Reward mean: 224.56198716322638, Reward std: 64.77979546380212\n",
            "Step: 275000, Reward mean: 23.40388344949906, Reward std: 96.16325433331943\n",
            "Step: 280000, Reward mean: 102.38567890024316, Reward std: 142.10804702631103\n",
            "Step: 285000, Reward mean: 180.16308649960015, Reward std: 98.1781741569053\n",
            "Step: 290000, Reward mean: 113.3759144971636, Reward std: 122.30554517821592\n",
            "Step: 295000, Reward mean: 211.03380427471762, Reward std: 28.775053400037\n",
            "Step: 300000, Reward mean: 130.99087968460694, Reward std: 115.17679830414414\n",
            "Step: 305000, Reward mean: 151.40920013230826, Reward std: 106.7943711364527\n",
            "Step: 310000, Reward mean: 220.14034675666989, Reward std: 52.96626300453224\n",
            "Step: 315000, Reward mean: 132.9480733421859, Reward std: 106.75566415358435\n",
            "Step: 320000, Reward mean: 29.276865196119694, Reward std: 151.22446664989738\n",
            "Step: 325000, Reward mean: 85.61886112552376, Reward std: 100.87101283281314\n",
            "Step: 330000, Reward mean: 131.81898575494907, Reward std: 80.46984234201051\n",
            "Step: 335000, Reward mean: 179.55724051117892, Reward std: 166.4953885763023\n",
            "Step: 340000, Reward mean: 225.20740393427053, Reward std: 62.659074957615275\n",
            "Step: 345000, Reward mean: 237.44551461186265, Reward std: 30.798145700833857\n",
            "Step: 350000, Reward mean: 231.56867963668014, Reward std: 36.00899272246305\n",
            "Step: 355000, Reward mean: 246.91998748150831, Reward std: 41.14267757913275\n",
            "Step: 360000, Reward mean: -208.17128334055823, Reward std: 303.2116095462755\n",
            "Step: 365000, Reward mean: 183.953145017953, Reward std: 103.93045996576674\n",
            "Step: 370000, Reward mean: 258.46763965069425, Reward std: 21.11832421769292\n",
            "Step: 375000, Reward mean: 251.64037065223792, Reward std: 9.746552923311627\n",
            "Step: 380000, Reward mean: 238.6340318010225, Reward std: 66.26035315479595\n",
            "Step: 385000, Reward mean: 250.59418592478124, Reward std: 24.46659976671336\n",
            "Step: 390000, Reward mean: 61.47073080119391, Reward std: 231.16297927855408\n",
            "Step: 395000, Reward mean: 181.974405347982, Reward std: 154.65583954339985\n",
            "Step: 400000, Reward mean: 252.3550827239761, Reward std: 20.087003673566123\n",
            "Step: 405000, Reward mean: 266.48975203083694, Reward std: 20.161229549343958\n",
            "Step: 410000, Reward mean: 236.2372004705133, Reward std: 7.891996776710251\n",
            "Step: 415000, Reward mean: 109.95055561699573, Reward std: 190.38388700268408\n",
            "Step: 420000, Reward mean: 192.73437413192596, Reward std: 84.69340085654197\n",
            "Step: 425000, Reward mean: 191.66658105436784, Reward std: 106.81019802107339\n",
            "Step: 430000, Reward mean: 53.64394129295713, Reward std: 215.26003465905367\n",
            "Step: 435000, Reward mean: 248.1462231577244, Reward std: 70.6368433402598\n",
            "Step: 440000, Reward mean: 210.12231499371416, Reward std: 122.73605190138699\n",
            "Step: 445000, Reward mean: 104.27490767979812, Reward std: 211.30005461982626\n",
            "Step: 450000, Reward mean: 261.69137357516723, Reward std: 14.80438079253295\n",
            "Step: 455000, Reward mean: 37.09118175811112, Reward std: 208.00126626639994\n",
            "Step: 460000, Reward mean: 239.1752002054362, Reward std: 49.142330542700066\n",
            "Step: 465000, Reward mean: 149.91202929278813, Reward std: 195.94804070307313\n",
            "Step: 470000, Reward mean: -33.47987410110146, Reward std: 251.43320646066633\n",
            "Step: 475000, Reward mean: -108.99834943847661, Reward std: 233.75247906286677\n",
            "Step: 480000, Reward mean: 174.27633482725244, Reward std: 157.23937493405288\n",
            "Step: 485000, Reward mean: 241.50938029700336, Reward std: 45.25851104012756\n",
            "Step: 490000, Reward mean: 258.3762824568026, Reward std: 13.80725403998975\n",
            "Step: 495000, Reward mean: 210.4560648072076, Reward std: 48.99253951608686\n",
            "Step: 500000, Reward mean: 255.0051494578394, Reward std: 25.602280591671978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xVj7qewedvjf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
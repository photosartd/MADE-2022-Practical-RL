{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RySu8w8IC8Cu",
        "outputId": "fad3a2bf-d956-423c-a5c5-328833f2f061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (91.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 91.7 MB 1.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.5\n"
          ]
        }
      ],
      "source": [
        "!pip install pybullet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "VWyFCPeMC18S"
      },
      "outputs": [],
      "source": [
        "import pybullet_envs\n",
        "from gym import make\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import Adam\n",
        "import random\n",
        "import copy\n",
        "\n",
        "GAMMA = 0.99\n",
        "TAU = 0.005\n",
        "CRITIC_LR = 3e-4\n",
        "ACTOR_LR = 3e-4\n",
        "NOISE = 0.2\n",
        "NOISE_CLIP = 0.5\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 256\n",
        "ENV_NAME = \"AntBulletEnv-v0\"\n",
        "TRANSITIONS = 1000000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fKZB03XBKGjH",
        "outputId": "2fe4142e-26af-4e13-9626-b19c04c6d8a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6Uj6Xot0DJG4"
      },
      "outputs": [],
      "source": [
        "def soft_update(target, source):\n",
        "    for tp, sp in zip(target.parameters(), source.parameters()):\n",
        "        tp.data.copy_((1 - TAU) * tp.data + TAU * sp.data)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(state_dim, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, action_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        \n",
        "    def forward(self, state):\n",
        "        return self.model(state)\n",
        "        \n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, state, action):\n",
        "        return self.model(torch.cat([state, action], dim=-1)).view(-1)\n",
        "\n",
        "\n",
        "class TD3:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.step = 0\n",
        "        self.action_dim = action_dim\n",
        "        self.actor = Actor(state_dim, action_dim).to(DEVICE)\n",
        "        self.critic_1 = Critic(state_dim, action_dim).to(DEVICE)\n",
        "        self.critic_2 = Critic(state_dim, action_dim).to(DEVICE)\n",
        "        \n",
        "        self.actor_optim = Adam(self.actor.parameters(), lr=ACTOR_LR)\n",
        "        self.critic_1_optim = Adam(self.critic_1.parameters(), lr=ACTOR_LR)\n",
        "        self.critic_2_optim = Adam(self.critic_2.parameters(), lr=ACTOR_LR)\n",
        "        \n",
        "        self.target_actor = copy.deepcopy(self.actor)\n",
        "        self.target_critic_1 = copy.deepcopy(self.critic_1)\n",
        "        self.target_critic_2 = copy.deepcopy(self.critic_2)\n",
        "        \n",
        "        self.replay_buffer = deque(maxlen=200000)\n",
        "\n",
        "    def update(self, transition):\n",
        "        self.replay_buffer.append(transition)\n",
        "        if len(self.replay_buffer) > BATCH_SIZE * 16:\n",
        "            \n",
        "            # Sample batch\n",
        "            transitions = [self.replay_buffer[random.randint(0, len(self.replay_buffer)-1)] for _ in range(BATCH_SIZE)]\n",
        "            state, action, next_state, reward, done = zip(*transitions)\n",
        "            state = torch.tensor(np.array(state), device=DEVICE, dtype=torch.float)\n",
        "            action = torch.tensor(np.array(action), device=DEVICE, dtype=torch.float)\n",
        "            next_state = torch.tensor(np.array(next_state), device=DEVICE, dtype=torch.float)\n",
        "            reward = torch.tensor(np.array(reward), device=DEVICE, dtype=torch.float)\n",
        "            done = torch.tensor(np.array(done), device=DEVICE, dtype=torch.float)\n",
        "            \n",
        "            # Update critic\n",
        "            with torch.no_grad():\n",
        "                noise = (\n",
        "                    torch.randn_like(action) * NOISE\n",
        "                ).clamp(-NOISE_CLIP, NOISE_CLIP)\n",
        "                \n",
        "                next_action = (self.target_actor(next_state) + noise).clamp(-1, 1)\n",
        "\n",
        "                q_target_1 = self.target_critic_1(next_state, next_action)\n",
        "                q_target_2 = self.target_critic_2(next_state, next_action)\n",
        "                q_target = torch.min(q_target_1, q_target_2)\n",
        "                q_target = reward + (1 - done) * GAMMA * q_target\n",
        "            # Get current Q estimates\n",
        "            q_curr_1 = self.critic_1(state, action)\n",
        "            q_curr_2 = self.critic_2(state, action)\n",
        "\n",
        "            # Compute critic loss\n",
        "            critic_loss = F.mse_loss(q_curr_1, q_target) + F.mse_loss(q_curr_2, q_target)\n",
        "            self.critic_1_optim.zero_grad()\n",
        "            self.critic_2_optim.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.critic_1_optim.step()\n",
        "            self.critic_2_optim.step()\n",
        "            \n",
        "            if self.step % 2 == 0:\n",
        "                # Update actor\n",
        "                actor_loss = -self.critic_1(state, self.actor(state)).mean()\n",
        "                self.actor_optim.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optim.step()\n",
        "                \n",
        "                soft_update(self.target_critic_1, self.critic_1)\n",
        "                soft_update(self.target_critic_2, self.critic_2)\n",
        "                soft_update(self.target_actor, self.actor)\n",
        "            self.step += 1\n",
        "\n",
        "    def act(self, state):\n",
        "        with torch.no_grad():\n",
        "            state = torch.tensor(np.array([state]), dtype=torch.float, device=DEVICE)\n",
        "            return self.actor(state).cpu().numpy()[0]\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.actor.state_dict(), \"agent.pt\")\n",
        "\n",
        "\n",
        "def evaluate_policy(env, agent, episodes=5):\n",
        "    returns = []\n",
        "    for _ in range(episodes):\n",
        "        done = False\n",
        "        state = env.reset()\n",
        "        total_reward = 0.\n",
        "        \n",
        "        while not done:\n",
        "            state, reward, done, _ = env.step(agent.act(state))\n",
        "            total_reward += reward\n",
        "        returns.append(total_reward)\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np6krdmVDONn",
        "outputId": "f12ef188-2617-456f-d70b-3617cc8e7bc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:175: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  \"Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:191: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  \"Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:196: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  \"Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 10000, Reward mean: 823.6651511056167, Reward std: 68.9889077710921\n",
            "Step: 20000, Reward mean: 417.0514512907721, Reward std: 91.44780347592332\n",
            "Step: 30000, Reward mean: 635.1486008710741, Reward std: 39.16137794665174\n",
            "Step: 40000, Reward mean: 503.00647130109064, Reward std: 20.498248828465474\n",
            "Step: 50000, Reward mean: 635.951560430877, Reward std: 95.86026994028924\n",
            "Step: 60000, Reward mean: 667.6050148721954, Reward std: 88.41046195158626\n",
            "Step: 70000, Reward mean: 323.6541433178499, Reward std: 210.62134849551862\n",
            "Step: 80000, Reward mean: 715.8696499501409, Reward std: 80.95972528703433\n",
            "Step: 90000, Reward mean: 546.5870284045623, Reward std: 75.39818396894313\n",
            "Step: 100000, Reward mean: 616.125995510998, Reward std: 37.70215388774769\n",
            "Step: 110000, Reward mean: 626.1739509273614, Reward std: 102.92350758546945\n",
            "Step: 120000, Reward mean: 415.73375220213836, Reward std: 90.42867582332966\n",
            "Step: 130000, Reward mean: 371.8444087451471, Reward std: 121.47444377661728\n",
            "Step: 140000, Reward mean: 693.4697047824009, Reward std: 193.41875941939952\n",
            "Step: 150000, Reward mean: 551.6064899947517, Reward std: 71.99172674230405\n",
            "Step: 160000, Reward mean: 602.177483041345, Reward std: 42.137323501091764\n",
            "Step: 170000, Reward mean: 509.9433751532104, Reward std: 144.6494043596648\n",
            "Step: 180000, Reward mean: 587.1419812321403, Reward std: 100.3856501258439\n",
            "Step: 190000, Reward mean: 607.3401197922577, Reward std: 87.88104313455278\n",
            "Step: 200000, Reward mean: 554.6052624682782, Reward std: 104.83242662448879\n",
            "Step: 210000, Reward mean: 608.9965493488879, Reward std: 122.38201465022338\n",
            "Step: 220000, Reward mean: 502.32080496441415, Reward std: 78.45655802396361\n",
            "Step: 230000, Reward mean: 527.4770133079362, Reward std: 29.43771537244583\n",
            "Step: 240000, Reward mean: 576.0322497074427, Reward std: 91.11406821483762\n",
            "Step: 250000, Reward mean: 509.58463381775283, Reward std: 42.93182979041323\n",
            "Step: 260000, Reward mean: 515.0043924893005, Reward std: 111.51542818044665\n",
            "Step: 270000, Reward mean: 544.6150880960197, Reward std: 81.32261619267051\n",
            "Step: 280000, Reward mean: 531.1179615042836, Reward std: 103.83878049829684\n",
            "Step: 290000, Reward mean: 471.63815159632077, Reward std: 112.31878460858829\n",
            "Step: 300000, Reward mean: 737.8617324369508, Reward std: 72.59530058317381\n",
            "Step: 310000, Reward mean: 714.0441501142144, Reward std: 117.97623432616986\n",
            "Step: 320000, Reward mean: 460.1731651186994, Reward std: 114.0079036293903\n",
            "Step: 330000, Reward mean: 619.5413785403514, Reward std: 95.72884463770642\n",
            "Step: 340000, Reward mean: 415.9970404417498, Reward std: 69.31698623678459\n",
            "Step: 350000, Reward mean: 628.226336571843, Reward std: 92.59552964801637\n",
            "Step: 360000, Reward mean: 576.7209556452958, Reward std: 132.9389745550895\n",
            "Step: 370000, Reward mean: 459.7637976818284, Reward std: 124.57706159165579\n",
            "Step: 380000, Reward mean: 601.0355173037125, Reward std: 105.01679424117933\n",
            "Step: 390000, Reward mean: 618.0879516968613, Reward std: 201.13659573431445\n",
            "Step: 400000, Reward mean: 715.0632568565262, Reward std: 74.21751657633041\n",
            "Step: 410000, Reward mean: 732.0484598533437, Reward std: 54.81958613562782\n",
            "Step: 420000, Reward mean: 674.0300612040376, Reward std: 38.55022721887747\n",
            "Step: 430000, Reward mean: 613.7359360682465, Reward std: 90.7430212377481\n",
            "Step: 440000, Reward mean: 647.1700019923207, Reward std: 129.0280798431162\n",
            "Step: 450000, Reward mean: 756.8712732133033, Reward std: 128.2048989617186\n",
            "Step: 460000, Reward mean: 688.0148239808158, Reward std: 86.34112219229006\n",
            "Step: 470000, Reward mean: 755.7806688535054, Reward std: 42.35901439169374\n",
            "Step: 480000, Reward mean: 667.1547461427083, Reward std: 101.20820301764242\n",
            "Step: 490000, Reward mean: 786.9365277267264, Reward std: 95.40237072894615\n",
            "Step: 500000, Reward mean: 671.1353938382273, Reward std: 168.124379541916\n",
            "Step: 510000, Reward mean: 755.0174792159486, Reward std: 108.51041426612761\n",
            "Step: 520000, Reward mean: 496.82449953636007, Reward std: 287.73111164032105\n",
            "Step: 530000, Reward mean: 787.3782816495016, Reward std: 140.78394289712145\n",
            "Step: 540000, Reward mean: 920.7987202852643, Reward std: 98.86400810510607\n",
            "Step: 550000, Reward mean: 1085.6566415900268, Reward std: 103.845882288091\n",
            "Step: 560000, Reward mean: 1030.0331377296952, Reward std: 159.35666904310423\n",
            "Step: 570000, Reward mean: 982.8914424838136, Reward std: 89.49331110363156\n",
            "Step: 580000, Reward mean: 1222.0910209586193, Reward std: 158.44073650117565\n",
            "Step: 590000, Reward mean: 803.2481096232461, Reward std: 161.9096432461903\n",
            "Step: 600000, Reward mean: 1024.3506027629314, Reward std: 231.9706552937599\n",
            "Step: 610000, Reward mean: 1145.7098203966439, Reward std: 237.33294780409457\n",
            "Step: 620000, Reward mean: 1009.2777780369224, Reward std: 322.44488730724606\n",
            "Step: 630000, Reward mean: 1581.38536246312, Reward std: 31.62914522324615\n",
            "Step: 640000, Reward mean: 1483.2190087760926, Reward std: 17.110385016950648\n",
            "Step: 650000, Reward mean: 1488.370048424134, Reward std: 206.61925677167102\n",
            "Step: 660000, Reward mean: 1107.0124651515373, Reward std: 346.5747701172302\n",
            "Step: 670000, Reward mean: 1565.29526754651, Reward std: 172.47122167930584\n",
            "Step: 680000, Reward mean: 1619.6915670219746, Reward std: 16.47846469404192\n",
            "Step: 690000, Reward mean: 1643.717291018797, Reward std: 17.61776424416235\n",
            "Step: 700000, Reward mean: 1639.9001050135164, Reward std: 312.1224657189512\n",
            "Step: 710000, Reward mean: 1756.4313662008312, Reward std: 35.91239429542613\n",
            "Step: 720000, Reward mean: 1430.954682333581, Reward std: 59.037998411963684\n",
            "Step: 730000, Reward mean: 1405.39589418048, Reward std: 181.83317367991955\n",
            "Step: 740000, Reward mean: 1656.648307191405, Reward std: 44.53283983920678\n",
            "Step: 750000, Reward mean: 1794.646909238673, Reward std: 21.676121034438896\n",
            "Step: 760000, Reward mean: 1768.2204784558805, Reward std: 49.23032648463035\n",
            "Step: 770000, Reward mean: 1454.3231002312004, Reward std: 419.9886873228796\n",
            "Step: 780000, Reward mean: 1731.4991667154532, Reward std: 18.064402795949995\n",
            "Step: 790000, Reward mean: 968.9235649620026, Reward std: 315.54545146216765\n",
            "Step: 800000, Reward mean: 1700.9438885400082, Reward std: 38.831492004378354\n",
            "Step: 810000, Reward mean: 1485.0200482513887, Reward std: 552.5390821206662\n",
            "Step: 820000, Reward mean: 1373.7547212456818, Reward std: 423.3798567160198\n",
            "Step: 830000, Reward mean: 1442.7993844793718, Reward std: 503.1730259300197\n",
            "Step: 840000, Reward mean: 1678.480103230611, Reward std: 273.34371514827757\n",
            "Step: 850000, Reward mean: 1823.03812950163, Reward std: 29.74270414007527\n",
            "Step: 860000, Reward mean: 1876.5614864711847, Reward std: 94.14633952398151\n",
            "Step: 870000, Reward mean: 1808.7791815077467, Reward std: 23.367966965514857\n",
            "Step: 880000, Reward mean: 1727.6294333887022, Reward std: 110.38277775747355\n",
            "Step: 890000, Reward mean: 1987.9889183346186, Reward std: 28.12568515124272\n",
            "Step: 900000, Reward mean: 1948.575289331926, Reward std: 45.21285045130481\n",
            "Step: 910000, Reward mean: 1302.5856575394284, Reward std: 171.84383562226816\n",
            "Step: 920000, Reward mean: 1618.9656445232104, Reward std: 231.43287262590368\n",
            "Step: 930000, Reward mean: 1984.3657452683783, Reward std: 33.970246960429265\n",
            "Step: 940000, Reward mean: 1815.1834405963837, Reward std: 57.700827842249744\n",
            "Step: 950000, Reward mean: 1887.2763892337334, Reward std: 66.59163039951143\n",
            "Step: 960000, Reward mean: 1753.5339616189583, Reward std: 403.2279951119569\n",
            "Step: 970000, Reward mean: 1983.589811018114, Reward std: 29.42289375122388\n",
            "Step: 980000, Reward mean: 1486.5195410387264, Reward std: 481.02450895257033\n",
            "Step: 990000, Reward mean: 1146.1907273897923, Reward std: 695.6823702889111\n",
            "Step: 1000000, Reward mean: 1909.3110369611502, Reward std: 31.36557779515358\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    env = make(ENV_NAME)\n",
        "    test_env = make(ENV_NAME)\n",
        "    td3 = TD3(state_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0])\n",
        "    state = env.reset()\n",
        "    episodes_sampled = 0\n",
        "    steps_sampled = 0\n",
        "    eps = 0.3\n",
        "    eps_final = 0.01\n",
        "    eps_decay = 40000\n",
        "    best_mean_reward = 0\n",
        "    for i in range(TRANSITIONS):\n",
        "        steps = 0\n",
        "        \n",
        "        #Epsilon-greedy policy\n",
        "        current_eps = eps + (eps_final - eps) * i / eps_decay\n",
        "        current_eps = current_eps if current_eps >= eps_final else eps_final\n",
        "        action = td3.act(state)\n",
        "        action = np.clip(action + current_eps * np.random.randn(*action.shape), -1, +1)\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        td3.update((state, action, next_state, reward, done))\n",
        "        \n",
        "        state = next_state if not done else env.reset()\n",
        "        \n",
        "        if (i + 1) % (TRANSITIONS//100) == 0:\n",
        "            rewards = evaluate_policy(test_env, td3, 5)\n",
        "            curr_mean_reward = np.mean(rewards)\n",
        "            print(f\"Step: {i+1}, Reward mean: {curr_mean_reward}, Reward std: {np.std(rewards)}\")\n",
        "            if curr_mean_reward > best_mean_reward:\n",
        "                best_mean_reward = curr_mean_reward\n",
        "                td3.save()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_mean_reward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4q3Hx5A-Alw",
        "outputId": "6ca033a4-c53c-4040-dad4-65a7fd48e306"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1987.9889183346186"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHPrDTSaInDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "319dda25-4805-4a93-9d40-55b54ea8abaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 10000, Reward mean: 1302.7541447668261, Reward std: 670.4789759354582\n",
            "Step: 20000, Reward mean: 1724.5521297061223, Reward std: 387.96100960828664\n",
            "Step: 30000, Reward mean: 2069.7823975149886, Reward std: 64.99273975767048\n",
            "Step: 40000, Reward mean: 2171.1697182519547, Reward std: 19.185764719687477\n",
            "Step: 50000, Reward mean: 1973.9572492438958, Reward std: 39.73270416976789\n",
            "Step: 60000, Reward mean: 951.2848406605632, Reward std: 187.80481065936905\n",
            "Step: 70000, Reward mean: 2084.256168206799, Reward std: 31.690838274740496\n",
            "Step: 80000, Reward mean: 1894.1454974606488, Reward std: 138.98636907478698\n",
            "Step: 90000, Reward mean: 1988.284556893693, Reward std: 70.69332672906621\n",
            "Step: 100000, Reward mean: 2139.6838390305866, Reward std: 34.7676465878369\n",
            "Step: 110000, Reward mean: 1875.7850006866743, Reward std: 425.8956761319998\n",
            "Step: 120000, Reward mean: 1604.5491780821196, Reward std: 483.7878310103452\n",
            "Step: 130000, Reward mean: 1558.7788996211577, Reward std: 235.28109449424312\n",
            "Step: 140000, Reward mean: 1040.4128487406565, Reward std: 609.7989280746663\n",
            "Step: 150000, Reward mean: 2226.3985878848175, Reward std: 23.341131752600003\n",
            "Step: 160000, Reward mean: 2145.570317974954, Reward std: 18.841403579689988\n",
            "Step: 170000, Reward mean: 2212.8247632609264, Reward std: 18.592557127687638\n",
            "Step: 180000, Reward mean: 2098.0725366665656, Reward std: 45.30635066917705\n",
            "Step: 190000, Reward mean: 1940.9793191613855, Reward std: 164.89040316919258\n",
            "Step: 200000, Reward mean: 1597.432693411998, Reward std: 616.1890108316614\n",
            "Step: 210000, Reward mean: 1599.3119415919134, Reward std: 659.4615584807046\n",
            "Step: 220000, Reward mean: 2157.321784397489, Reward std: 30.164883256672656\n",
            "Step: 230000, Reward mean: 1351.164535659174, Reward std: 706.4309299367488\n",
            "Step: 240000, Reward mean: 1882.5398065950153, Reward std: 397.9330239332692\n",
            "Step: 250000, Reward mean: 1813.9989066501225, Reward std: 736.5516224695136\n",
            "Step: 260000, Reward mean: 1869.3266868750725, Reward std: 766.9568373336194\n",
            "Step: 270000, Reward mean: 1598.4708438396951, Reward std: 957.7651064742467\n",
            "Step: 280000, Reward mean: 2244.7342256385555, Reward std: 32.95596858624056\n",
            "Step: 290000, Reward mean: 2369.9413357020717, Reward std: 61.70114270897865\n",
            "Step: 300000, Reward mean: 1771.1831681228803, Reward std: 614.2329735445744\n",
            "Step: 310000, Reward mean: 1906.2755393657676, Reward std: 694.0107960393318\n",
            "Step: 320000, Reward mean: 2222.384854271194, Reward std: 49.66894871645287\n",
            "Step: 330000, Reward mean: 2316.490791065058, Reward std: 16.810706863041776\n",
            "Step: 340000, Reward mean: 1828.5540498715068, Reward std: 644.6158260696703\n",
            "Step: 350000, Reward mean: 1791.7694908767724, Reward std: 556.1733661726619\n",
            "Step: 360000, Reward mean: 1591.9853797098835, Reward std: 717.2181850438228\n",
            "Step: 370000, Reward mean: 1435.8821717855676, Reward std: 738.4217934152863\n",
            "Step: 380000, Reward mean: 1545.2034983380388, Reward std: 791.5440753811974\n",
            "Step: 390000, Reward mean: 2058.6331315606803, Reward std: 241.68079884377204\n",
            "Step: 400000, Reward mean: 2340.9970601052673, Reward std: 59.97847683719706\n",
            "Step: 410000, Reward mean: 1630.6993848563284, Reward std: 590.0362331684873\n",
            "Step: 420000, Reward mean: 2312.2869109608796, Reward std: 40.73289487698744\n",
            "Step: 430000, Reward mean: 1439.0350198385108, Reward std: 1023.6823970416283\n",
            "Step: 440000, Reward mean: 2456.1355150508707, Reward std: 64.67406284191024\n",
            "Step: 450000, Reward mean: 868.231827853092, Reward std: 676.5518081864051\n",
            "Step: 460000, Reward mean: 2202.9319760441513, Reward std: 28.830931257504012\n",
            "Step: 470000, Reward mean: 1131.2454715026483, Reward std: 942.5762038448698\n",
            "Step: 480000, Reward mean: 2274.3484433548583, Reward std: 22.764770200249178\n",
            "Step: 490000, Reward mean: 2352.1991456890014, Reward std: 23.379091978448148\n",
            "Step: 500000, Reward mean: 1504.2797616566993, Reward std: 975.375758724826\n",
            "Step: 510000, Reward mean: 2405.3434738944147, Reward std: 15.76756276329246\n",
            "Step: 520000, Reward mean: 2245.744633770447, Reward std: 16.578576293868316\n",
            "Step: 530000, Reward mean: 2292.8640846897933, Reward std: 68.59483468012306\n",
            "Step: 540000, Reward mean: 2391.4094355007423, Reward std: 19.595774329617658\n",
            "Step: 550000, Reward mean: 1600.4781029180363, Reward std: 737.0108425482578\n",
            "Step: 560000, Reward mean: 1997.640098344104, Reward std: 705.7926700178965\n",
            "Step: 570000, Reward mean: 2346.1030028271953, Reward std: 22.228222384241093\n",
            "Step: 580000, Reward mean: 2410.9671388385505, Reward std: 16.577723917057074\n",
            "Step: 590000, Reward mean: 1718.599786966311, Reward std: 758.8477370098038\n",
            "Step: 600000, Reward mean: 535.6968897176934, Reward std: 55.48983441897535\n",
            "Step: 610000, Reward mean: 504.31268156560674, Reward std: 172.59758137494293\n",
            "Step: 620000, Reward mean: 868.5737128870171, Reward std: 248.64371254742588\n",
            "Step: 630000, Reward mean: 2281.330129557132, Reward std: 20.258789809700193\n",
            "Step: 640000, Reward mean: 2056.9244633929006, Reward std: 736.4898935857834\n",
            "Step: 650000, Reward mean: 1959.625337302497, Reward std: 791.6318699744837\n",
            "Step: 660000, Reward mean: 1629.874273631803, Reward std: 982.2071220069555\n",
            "Step: 670000, Reward mean: 2012.068261098903, Reward std: 675.8624010061875\n",
            "Step: 680000, Reward mean: 1962.282489021371, Reward std: 792.6951092710207\n",
            "Step: 690000, Reward mean: 1983.7554302531366, Reward std: 858.0296605559671\n",
            "Step: 700000, Reward mean: 2209.2121385478795, Reward std: 148.12733156294962\n",
            "Step: 710000, Reward mean: 1463.9678776297617, Reward std: 868.2852097179018\n",
            "Step: 720000, Reward mean: 2386.8832267964335, Reward std: 17.156938302904955\n",
            "Step: 730000, Reward mean: 2442.3296306808493, Reward std: 11.757272735024092\n",
            "Step: 740000, Reward mean: 2315.253508814056, Reward std: 230.79192485520613\n",
            "Step: 750000, Reward mean: 1962.686845726453, Reward std: 820.085893798583\n",
            "Step: 760000, Reward mean: 2053.02366031692, Reward std: 678.7040722810399\n",
            "Step: 770000, Reward mean: 2210.040811659568, Reward std: 159.08110699555178\n",
            "Step: 780000, Reward mean: 2410.5907555860513, Reward std: 23.39966327867473\n",
            "Step: 790000, Reward mean: 2383.1920419136914, Reward std: 37.784401878970606\n",
            "Step: 800000, Reward mean: 2461.9868448630036, Reward std: 33.15529883950864\n",
            "Step: 810000, Reward mean: 2457.2039139764975, Reward std: 22.232348970185413\n",
            "Step: 820000, Reward mean: 2495.4553892027666, Reward std: 17.694397388296014\n",
            "Step: 830000, Reward mean: 2411.924303689185, Reward std: 29.919885188134252\n",
            "Step: 840000, Reward mean: 2506.4998186566477, Reward std: 27.925413842365867\n",
            "Step: 850000, Reward mean: 2335.066443760053, Reward std: 49.20601566541759\n",
            "Step: 860000, Reward mean: 1763.192719724006, Reward std: 549.657205523987\n",
            "Step: 870000, Reward mean: 782.9604887822036, Reward std: 768.2394009568854\n",
            "Step: 880000, Reward mean: 317.8630726418374, Reward std: 114.61536306751893\n",
            "Step: 890000, Reward mean: 502.19090618533755, Reward std: 232.0326150976885\n",
            "Step: 900000, Reward mean: 1359.31150207346, Reward std: 832.466750817375\n",
            "Step: 910000, Reward mean: 2306.2181410179264, Reward std: 109.32984437084117\n",
            "Step: 920000, Reward mean: 2325.2383970805595, Reward std: 43.71673735492173\n",
            "Step: 930000, Reward mean: 2388.921940318665, Reward std: 51.81652192233739\n",
            "Step: 940000, Reward mean: 2408.8436812370483, Reward std: 31.502066568985075\n",
            "Step: 950000, Reward mean: 1682.3152822503566, Reward std: 875.0565606381008\n",
            "Step: 960000, Reward mean: 2358.1720626393844, Reward std: 18.674433350477468\n",
            "Step: 970000, Reward mean: 2456.8889827633784, Reward std: 39.88719516235966\n"
          ]
        }
      ],
      "source": [
        "env = make(ENV_NAME)\n",
        "#test_env = make(ENV_NAME)\n",
        "#td3 = TD3(state_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0])\n",
        "#state = env.reset()\n",
        "#episodes_sampled = 0\n",
        "#steps_sampled = 0\n",
        "eps = 0.03\n",
        "eps_final = 0.01\n",
        "eps_decay = 40000\n",
        "best_mean_reward = 0\n",
        "for i in range(TRANSITIONS):\n",
        "    steps = 0\n",
        "    \n",
        "    #Epsilon-greedy policy\n",
        "    current_eps = eps\n",
        "    action = td3.act(state)\n",
        "    action = np.clip(action + current_eps * np.random.randn(*action.shape), -1, +1)\n",
        "\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    td3.update((state, action, next_state, reward, done))\n",
        "    \n",
        "    state = next_state if not done else env.reset()\n",
        "    \n",
        "    if (i + 1) % (TRANSITIONS//100) == 0:\n",
        "        rewards = evaluate_policy(test_env, td3, 5)\n",
        "        curr_mean_reward = np.mean(rewards)\n",
        "        print(f\"Step: {i+1}, Reward mean: {curr_mean_reward}, Reward std: {np.std(rewards)}\")\n",
        "        if curr_mean_reward > best_mean_reward:\n",
        "            best_mean_reward = curr_mean_reward\n",
        "            td3.save()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_CMR-jWlMURW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}